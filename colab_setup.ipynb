{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Service PoC - Colab Setup\n",
        "\n",
        "This notebook sets up and runs the embedding service comparison on Colab with T4 GPU.\n",
        "\n",
        "**Runtime: GPU (T4)**"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Check GPU"
      ],
      "metadata": {
        "id": "check_gpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "nvidia_smi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Clone Repository"
      ],
      "metadata": {
        "id": "clone_repo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/YOUR_USERNAME/embedding_service_poc.git\n",
        "%cd embedding_service_poc"
      ],
      "metadata": {
        "id": "clone"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Install Dependencies"
      ],
      "metadata": {
        "id": "install_deps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q fastapi uvicorn pydantic tyro\n",
        "!pip install -q langchain langchain-community\n",
        "!pip install -q sentence-transformers transformers\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q httpx numpy datasets psutil\n",
        "!pip install -q structlog colorama\n",
        "!pip install -q vllm"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Prepare Test Dataset"
      ],
      "metadata": {
        "id": "prepare_data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python data_loader.py"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Test HuggingFace Backend"
      ],
      "metadata": {
        "id": "test_hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start service in background\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start HuggingFace service\n",
        "hf_process = subprocess.Popen(['python', 'service_huggingface.py'])\n",
        "print(\"Waiting for service to start...\")\n",
        "time.sleep(30)\n",
        "print(\"Service should be ready!\")"
      ],
      "metadata": {
        "id": "start_hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Health check\n",
        "import requests\n",
        "response = requests.get(\"http://localhost:8000/health\")\n",
        "print(response.json())"
      ],
      "metadata": {
        "id": "health_hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run stress test (10 minutes)\n",
        "!python stress_test.py --service-url http://localhost:8000 --duration-minutes 10 --batch-size 32 --max-concurrent-requests 10"
      ],
      "metadata": {
        "id": "stress_hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop HuggingFace service\n",
        "hf_process.terminate()\n",
        "hf_process.wait()\n",
        "print(\"HuggingFace service stopped\")"
      ],
      "metadata": {
        "id": "stop_hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Test vLLM Backend"
      ],
      "metadata": {
        "id": "test_vllm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start vLLM service\n",
        "vllm_process = subprocess.Popen(['python', 'service_vllm.py'])\n",
        "print(\"Waiting for vLLM service to start...\")\n",
        "time.sleep(60)  # vLLM takes longer to initialize\n",
        "print(\"Service should be ready!\")"
      ],
      "metadata": {
        "id": "start_vllm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Health check\n",
        "response = requests.get(\"http://localhost:8001/health\")\n",
        "print(response.json())"
      ],
      "metadata": {
        "id": "health_vllm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run stress test (10 minutes)\n",
        "!python stress_test.py --service-url http://localhost:8001 --duration-minutes 10 --batch-size 32 --max-concurrent-requests 10"
      ],
      "metadata": {
        "id": "stress_vllm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop vLLM service\n",
        "vllm_process.terminate()\n",
        "vllm_process.wait()\n",
        "print(\"vLLM service stopped\")"
      ],
      "metadata": {
        "id": "stop_vllm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Compare Results"
      ],
      "metadata": {
        "id": "compare"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import glob\n",
        "\n",
        "# Load all result files\n",
        "result_files = sorted(glob.glob(\"stress_test_results_*.json\"))\n",
        "\n",
        "print(\"\\n=\" * 60)\n",
        "print(\"COMPARISON: HuggingFace vs vLLM\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, file in enumerate(result_files[-2:]):\n",
        "    with open(file) as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    backend = \"HuggingFace\" if i == 0 else \"vLLM\"\n",
        "    print(f\"\\n{backend} Backend:\")\n",
        "    print(f\"  Requests/sec: {results['requests_per_second']:.2f}\")\n",
        "    print(f\"  Success rate: {results['success_rate_percent']:.2f}%\")\n",
        "    print(f\"  P50 latency: {results['latency_metrics']['p50']:.4f}s\")\n",
        "    print(f\"  P90 latency: {results['latency_metrics']['p90']:.4f}s\")\n",
        "    print(f\"  P99 latency: {results['latency_metrics']['p99']:.4f}s\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ],
      "metadata": {
        "id": "compare_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Download Results"
      ],
      "metadata": {
        "id": "download"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download all result files\n",
        "for file in result_files:\n",
        "    files.download(file)"
      ],
      "metadata": {
        "id": "download_files"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
